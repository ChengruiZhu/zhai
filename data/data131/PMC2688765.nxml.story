Spontaneous mutations are key drivers of evolution and disease. In microbes, most mutations are deleterious, some are neutral (without significant impact), and a few are advantageous. Because deleterious mutations reduce fitness, there should be constant selection for antimutator mutations that reduce rates of spontaneous mutation. However, such reductions are necessarily achieved at some cost. Therefore, a mutation rate should converge evolutionarily on a value that reflects this trade-off. For DNA microbes, the observed genomic mutation rate is remarkably (and mysteriously) invariant, in the neighborhood of 0.003â€“0.004, with a range of less than two-fold despite huge variation per average base pair in organisms with a wide diversity of life histories. Would an environmental condition that increased the average deleterious impact of a mutation be balanced by additional investments in antimutator mutations? It is widely observed that many mutations with mild impacts become strongly deleterious at higher temperatures, so mutation rates were measured in two thermophiles, a bacterium and an archaeon. Remarkably, both displayed average mutation rates reduced by about five-fold from the characteristic mesophilic value, most of the decrease reflecting a 10-fold reduction in the rate of base substitutions.