Recognizing the actions of others from video sequences across changes in viewpoint, gait or illumination is a hallmark of human visual intelligence. A large number of studies have highlighted which areas in the human brain are involved in the processing of biological motion, while others have described how single neurons behave in response to videos of human actions. However, little is known about the computational necessities that shaped these neural mechanisms either through evolution or experience. In this paper, we test the hypothesis that this computational goal is the discrimination of action categories from complex video stimuli and across identity-preserving transformations. We show that, within the class of Spatiotemporal Convolutional Neural Networks (ST-CNN), deliberate model modifications leading to representations of videos that better support robust action discrimination, also produce representations that better match human neural data. Importantly, increasing model performance on invariant action recognition leads to a better match with human neural data, despite the model never being exposed to such data. These results suggest that, similarly to what is known for object recognition, supporting invariant discrimination within the constraints of hierarchical ST-CNN architectures drives the neural mechanisms underlying our ability to perceive the actions of others.