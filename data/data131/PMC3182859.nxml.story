The evolution of speech is one of our most fascinating and enduring mysteries—enduring partly because all the critical features of speech (brains, vocal tracts, ancestral speech-like sounds) do not fossilize. Furthermore, it is becoming increasingly clear that speech is, by default, a multimodal phenomenon: we use both faces and voices together to communicate. Thus, understanding the evolution of speech requires a comparative approach using closely-related extant primate species and recognition that vocal communication is audiovisual. Using computer-generated avatar faces, we compared the integration of faces and voices in monkeys and humans performing an identical detection task. Both species responded faster when faces and voices were presented together relative to the face or voice alone. While the details sometimes appeared to differ, the behavior of both species could be well explained by a “superposition” model positing the linear summation of activity patterns in response to visual and auditory components of vocalizations. Other, more popular computational models of multisensory integration failed to explain our data. Thus, the superposition model represents a putative homologous mechanism for integrating faces and voices across primate species.