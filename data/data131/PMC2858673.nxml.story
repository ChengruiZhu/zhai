Because our eyes are set apart horizontally in our head, the images they see are mainly offset horizontally. However, small vertical disparities also occur, and can have a measurable effect on perception, showing that they must be detected by the visual system. The trouble is that encoding a two-dimensional quantity is much more expensive for neuronal systems than encoding a one-dimensional quantity. This paper shows that, for two-dimensional disparity, the brain could potentially take advantage of a major simplification. This strategy would avoid the need to build neurons tuned to a range of vertical disparities at each retinotopic location. For example, at the centre of the visual field, vertical disparities are almost always zero. The brain could make sure all its neurons at this location respond best to zero vertical disparity, ensuring best performance for the most common disparities. But the brain would still know what the vertical disparity actually was, which would be useful on rare occasions where it was not zero, e.g., when the eyes are misaligned. This is an interesting example because usually, neuronal populations which are all tuned to the same value of a quantity cannot encode that quantity (e.g., a retina with only one type of cone cell cannot encode color).