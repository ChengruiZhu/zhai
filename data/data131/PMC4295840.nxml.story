Neural networks, whether artificial or biological, consist of individual units connected together that mutually send and receive parcels of energy called spikes. While simply described, there is a vast space of possible implementations, instantiations, and varieties of neural networks. Some of these networks are critically balanced between randomness and order, and between death by decay and death by explosion. Selecting just the right properties and parameters for a particular network to reach this critical state can be difficult and time-consuming. The strength of connections between units may change over time via synaptic plasticity, and we exploit this mechanism to create a network that self-tunes to criticality. More specifically, the interplay of opposing forces from excitatory and inhibitory plasticity create a balance that allows self-tuning to take place. This self-tuning takes relatively simple spiking units and connects them in a way that creates complex behavior. Our results have implications for the design of artificial neural networks implemented in hardware, where parameter tuning can be costly, but may provide insight into the critical nature of biological networks as well.